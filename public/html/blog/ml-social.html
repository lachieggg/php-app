<html>
	<head>
		<title>The dangers of machine learning powered social media platforms</title> 
	</head>


	<body>
		<p>
		<b>The dangers of machine learning powered social media platforms</b>
		<br>
		<br>
		I've been listening to a lot of the Lex Fridman podcast lately, particularly the interviews with Donald Knuth, Michael I. Jordan,
		David Silver, and Jaron Lanier. 
		<br>
		<br>
		It was an interesting progression in the sense that it was good to get some background on the latest developments in A.I. first,
		and then to add the formulated philosophical position that Lanier has. Especially in the context of social
		media use and the impacts it has on society. Lanier is a pretty interesting person, and seems to be somewhat of a polymath.
		While he has a rich understanding of Computer Science and the history of the Internet, he also has well formulated
		philosophical and moral perspectives that are so lacking in the technology industry today. 
		<br>
		<br>
		The consensus among various technologists with a moral compass seems to be that having machine learning algorithms that optimize for
		metrics like "time on site" and "ad revenue" ends up with some pretty severe consequences for society when you scale it to billions of people.
		It seems that the algorithms will tend to serve content that has certain negative emotions associated with it, because on average that
		drives more engagement on the platforms. From a psychological perspective this seems fairly obvious because humans have an inherent negativity bias,
		that is, we are prone to recall negative events more strongly than positive ones, and because we have a whole neurological system designed for
		responding to threatening stimuli.
		<br>
		<br>
		Machine learning algorithms are in most cases, opaque, in that the Engineers of such a system do not understand exactly why the algorithm
		will produce a particular result. Specifically, Artificial Neural Networks that are used in a variety of applications including social media,
		have what is called a "hidden layer", which is referred to as such because it is impossible to visualize or explain how the neural network
		arrives at a particular result given some set of inputs. For instance, we use neural networks to identify whether an image contains a cat or not,
		say for searching through a database of pictures on Google Images. In this case, the algorithm might give a yes or no response to the input image, 
		but it is not necessarily clear, even to the Engineer, why the algorithm produced that output.
		<br>
		<br>
		Bringing this back in to the topic of social media, companies use ML to work out what content they should serve up to their users to optimize for
		certain metrics, like how long a given user will spend on Facebook. That is different for every user, but every time a user loads up Facebook,
		even information like how long they spent on the site, what posts they liked, what posts they spent more or less time looking at,
		serve as useful data for the company in training their models to predict and potentially manipulate future behaviour.
		<br>
		<br>
		This is a very deep topic and spans multiple areas, and to cover it in detail is well beyond the medium of a blog post. But the main point
		is that advertising is the life blood of the social media platforms. They optimize their algorithms around ad revenue, and as a result of that,
		the content that they serve up is necessarily an inaccurate representation of the real world. The argument Lanier makes is that the reason the last 6 years have
		been filled with increased political unrest, is because these algorithms,
		which are oftentimes opaque to the Engineer, are serving up negative content.. That's not their mandate or what they are explicitly designed to do, but it is a 
		by product of a set of incentives that optimizes for time on site and not satisfaction and contentment.
		</p>
		 <a href="/blog">Back to blog</a>
		<br>
		<a href="/">Back to main site</a>

	</body>
</html>


